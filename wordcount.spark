import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD

object wordcount {

  def main(args: Array[String]) {

    val pathToFile = "log.txt"

    val conf = new SparkConf().setAppName("Wordcount").setMaster("local[*]")

    val sc = new SparkContext(conf)

    val wordsRdd = sc.textFile(pathToFile).flatMap(_.split(" "))

    val wordCountInitRdd = wordsRdd.map(word => (word, 1))

    val wordCountRdd = wordCountInitRdd.reduceByKey((v1, v2) => v1 + v2)
    
    val highFreqWords = wordCountRdd.filter(x => x._2 > 2)

    highFreqWords.saveAsTextFile("wordcountsDir")
  }
}

#wordcount.sbt
name := "WordCount"
 
version := "1.0"
 
scalaVersion := "2.11.12"
 
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.3.0"

#sbt package
#sbt compile
#sbt run
